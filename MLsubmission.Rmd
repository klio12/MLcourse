```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```
##Human Activity Recognition
by **Nebojsa Mirkovic**

###OVERVIEW
Conventional activity recognition tracks whether an activity is performed, while qualitative activity recognition focuses on *how well* the activity is performed. The quality of activity execution is defined as the adherence of the execution to a specification, and this comparison is performed by the qualitative activity recognition system, which detects any performance mistakes and provides (real-time) feedback to the user on how to correct them. One way to define activities is the *sensor-oriented approach*, where a classification algorithm is trained on the data obtained from wearable sensors that quantify various features of activities as they are being performed.  

The purpose of this assignment was to predict in which one of the 5 possible ways the activity of *Unilateral Dumbbell Biceps Curl* was performed. The training and testing datasets were based on the dataset that was first described in the paper *Velloso, E et al. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13). Stuttgart, Germany: ACM SIGCHI, 2013*. The authors of the study placed  the so-called 'inertial measurement units' (IMUs) in commonly used workout equipment (glove, armband, lumbar belt and dumbbell) to capture acceleration, gyroscope and magnetometer data. Six male volunteers performed 10 repetitions of the activity either correctly or in 4 different wrong ways, and the data was collected in overlapping time windows 0.5-2.5 seconds in duration.  

Initial testing data set was cleaned up to decrease the number of variables, and split into training and validation subsets in ratio 3:1. Various predictive algorithms were then trained with the goal of achieving high enough prediction accuracy on the validation subset before predicting on the training set:

* "Brute force" use of highly-accurate random forest algorithm with all variables ran too long  
* Simple classification tree algorithm using all variables proved inaccurate but allowed for ranking of variables by importance. The 10 variables of highest importance were provided to the random forest algorithm with a limited number of cross-validation rounds to create a highly accurate model in reasonable time  
* Bagging algorithm on classification tree, using all variables, achieved high accuracy at the expense of somewhat longer run time  
* SVM algorithm using all variables ran fast and was highly accurate  

All approaches yielded *the same* predictions on the test dataset, implying that the prediction power of each one of them was high and that they likely were not overtrained.

**NOTE**: all calculations were performed on Dell XPS laptop, with 8GB RAM and IntelCore i5-5300U dual-core processor @ 2.3GHz

### DATA SET CLEANUP AND SIMPLIFICATION

Training and test data sets were loaded into data frames.
```{r load}
tv = read.csv("pml-training.csv")
test = read.csv("pml-testing.csv")
```

Libraries were loaded
```{r caret, results='hide'}
if(!require(caret)){stop("Install package caret")}
library(caret)
if(!require(randomForest)){stop("Install package randomForest")}
library(randomForest)
if(!require(e1071)){stop("Install package e1071")}
library(e1071)
```

Training dataset consists of 19,622 observations, each with 160 features. Some features were obtained by measurements at individual time points ("primary" features). Other features were derived from the primary ones as an aggregate for each time window and as such were redundant in information content and also had only one value per window. They were removed from the dataset to simplify model building without loss of information.

NOTE: all outcome classes were well represented so there was no need to scaling.

**Step 1**: remove columns with predominantly (>50%) entries being NA or an empty string
```{r s1, cache=TRUE}
tv = tv[,-which(apply(apply(tv, 2, FUN = function (i){i=="" | is.na(i)==TRUE}), 2, sum)/dim(tv)[1]>.5)]
tv = tv[,-which(names(tv) %in% c("X", "user_name", "new_window", "num_window", "cvtd_timestamp", "raw_timestamp_part_1", "raw_timestamp_part_2"))]

test1=test[,-which(apply(apply(test, 2, FUN = function (i){i=="" | is.na(i)==TRUE}), 2, sum)/dim(test)[1]>.5)]
test1=test1[,-which(names(test1) %in% c("X", "user_name", "new_window", "num_window", "cvtd_timestamp", "raw_timestamp_part_1", "raw_timestamp_part_2"))]
```

**Step 2**: No remaining variables had near-zero variance
```{r s2, cache=TRUE}
nZV=nearZeroVar(tv, saveMetrics=T)
rownames(nZV[nZV$nzv==TRUE, ])
```

The out-of-sample error rate was estimated by cross-validation using simple *hold out* approach, where the initial training set was split data into a **training set** and a **validation set** (in 3:1 ratio)
```{r spl, cache=TRUE}
train =    tv[ createDataPartition(tv$classe, p=.75, list=F), ]
validate = tv[-createDataPartition(tv$classe, p=.75, list=F), ]
```

### RUNNING PREDICTIVE ALGORITHMS
The minimum number of correct predictions on the test set that is required to pass this assignment dictated the required prediction accuracy on the validation set. Assuming binomial distribution of the test predictions (independent, equiprobable trials), it would take at least 96% probability of individual prediction in order to be certain (>99% cummulative probability) that over 80% predictions would be correct. 
```{r prob, cache=TRUE, message = FALSE}
pbinom(size=20, q=16, prob=.96, lower.tail=F)
```

First, a **classification tree** was trained in *15 sec*, but the accuracy was low
```{r rpart, cache=TRUE, message = FALSE}
set.seed(234)
m1 = train(classe ~ ., data=train, method="rpart")
pV1 = predict(m1, validate)
confusionMatrix(pV1, validate$classe)[[3]][1]
```

However, the model allowed for ranking of variables by importance
```{r top, cache=TRUE}
vi = varImp(m1)$importance
vi$feature = rownames(vi)
data.frame(feature = vi[order(vi$Overall, decreasing=T),][1:10,2], overall = round(vi[order(vi$Overall, decreasing=T),][1:10,1],2))
```

Top 10 ranked variables were used to train a **random forest**, with only 3 rounds of cross-validation. The run time was around *80 sec* and reached a high level of accuracy.
```{r rf, cache=TRUE}
train2 = cbind(train[,which(names(train) %in% vi[order(vi$Overall, decreasing=T), ][1:10,2])], classe=train$classe)
m2 = train(data=train2, classe ~ ., method = "rf", trControl = trainControl(method="cv", number=3))
pV2 = predict(m2, validate)
confusionMatrix(pV2, validate$classe)[[3]][1]
```

Therefore, the **estimated out-of-sample error** is:
```{r oose, cache=TRUE, echo = FALSE}
1 - confusionMatrix(pV2, validate$classe)[[3]][1]
```

**NOTE 2**: when the 5 most important variables were passed to random forest with 3, 5 or 10 rounds of cross-validation the accuracy was up to .97

Another prediction approach that was attempted was classification tree with **bagging**, which achieved high accuracy at the expense of a longer run (*370 sec*)
```{r bag, cache=TRUE}
set.seed(456)
m3 = train(data=train, classe ~ ., method = "treebag")
pV3 = predict(m3, validate)
confusionMatrix(pV3, validate$classe)[[3]][1]
```

Finally, an **SVM** was trained with default options. This approach proved to be the fastest (*40 sec*), despite a small drop in accuracy
```{r svm, cache=TRUE}
set.seed(123)
m4 = svm(classe ~., data=train)
pV4 = predict(m4, validate)
confusionMatrix(pV4, validate$classe)[[3]][1]
```

Predictions of all three models on the test set were identical.
```{r test, cache=TRUE}
df = data.frame(paste(predict(m2, test1)), paste(predict(m2, test1)), paste(predict(m4, test1)))
colnames(df) = c("Random forest", "Bagging", "SVM")
df
```


